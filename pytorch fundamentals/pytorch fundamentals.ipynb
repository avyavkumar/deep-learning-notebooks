{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e8e218",
   "metadata": {},
   "source": [
    "# pytorch fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74d02e",
   "metadata": {},
   "source": [
    "Can define tensors in torch and operate on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2e170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.Tensor([5,3])\n",
    "Y = torch.Tensor([2,1])\n",
    "\n",
    "print(X*Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f06961b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0384, 0.5201, 0.8723, 0.8118, 0.3066],\n",
      "        [0.6937, 0.7547, 0.2891, 0.9737, 0.5963]])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.rand([2,5])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca2efc",
   "metadata": {},
   "source": [
    "Instead of a `reshape()` operation, in PyTorch there is a `view()` operation to reshape tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c60a056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0384, 0.5201, 0.8723, 0.8118, 0.3066, 0.6937, 0.7547, 0.2891, 0.9737,\n",
      "         0.5963]])\n"
     ]
    }
   ],
   "source": [
    "print(Y.view([1,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aec591",
   "metadata": {},
   "source": [
    "### data cleaning and operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "797cf8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd2ece32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a01ec",
   "metadata": {},
   "source": [
    "Get a glimpse of the data fetched and transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ccd0b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([8, 5, 2, 5, 8, 9, 6, 9, 1, 2])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0b5f762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reshape the image and print the data\n",
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a590e6",
   "metadata": {},
   "source": [
    "### define a CNN for classifying images into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3b566",
   "metadata": {},
   "source": [
    "Define the neural network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca9aea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28,64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)        \n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215bb126",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02317e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9000, grad_fn=<NllLossBackward>)\n",
      "tensor(-0.9762, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-0.9000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimiser = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad() # make the delta in gradiants zero before computing the new delta in gradiants\n",
    "        output = net(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59983e47",
   "metadata": {},
   "source": [
    "Evaluate the correctness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f70145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.949\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy is \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b0c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
